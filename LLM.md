# LLM

## Training

Having an enormous source of data is an expensive solution. Only big sompanies can afford it. ==> A lot of OpenSource models are trained via distillation. You have a big model that generates answers, you train a smaller model to try have the same answers. You train by imitation.

New idea to train multi-modal models: instead of making captions of images for training, have people speak for 60 seconds about the image.

Instead of drawing bounding boxes, we just "point" at objects. Makes it easier for the model to point to items & much faster to create a dataset.
